{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d04bd962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.67879441e-01]\n",
      " [1.23409804e-04]]\n",
      "[[3.72665317e-06]\n",
      " [1.83156389e-02]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" naiveBayes.py \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def naiveBayes(classes, learner, parameterised_function, train_data):\n",
    "    f = {}\n",
    "    parameters = {}\n",
    "    g = {}\n",
    "    for class_value in classes:\n",
    "        # Initialize parameters and functions for each class\n",
    "        parameters[class_value] = {}\n",
    "        # f is a dictionary that maps feature indices to their parameterized functions\n",
    "        f[class_value] = {}\n",
    "        # parameters is a dictionary that maps feature indices to their learned parameters\n",
    "        # parameters[class_value][feature] contains the learned parameters for the feature\n",
    "        # train_x is the training data for the current class\n",
    "        train_x = train_data[train_data[:, -1] == class_value][:, :-1] #Takes the features associated with datapoints in a class\n",
    "        for feature in range(train_x.shape[1]): \n",
    "            parameters[class_value][feature] = learner(train_x[:,feature])\n",
    "            # \n",
    "            f[class_value][feature] = parameterised_function(parameters[class_value][feature])\n",
    "        def create_g(class_value):     \n",
    "            def g(test_data):\n",
    "                unscaled_feature_likelihoods = np.array([\n",
    "                    [f[class_value][feature](test_data[point, feature]) for feature in range(test_data.shape[1])]\n",
    "                    for point in range(test_data.shape[0])\n",
    "                ])\n",
    "                unscaled_point_likelihood = np.prod(unscaled_feature_likelihoods, axis=1).reshape(-1, 1)\n",
    "                return unscaled_point_likelihood\n",
    "            return g\n",
    "        g[class_value] = create_g(class_value)\n",
    "    return g\n",
    "\n",
    "classes = [0,1]\n",
    "def learner(train):\n",
    "    mu = np.mean(train)\n",
    "    sig = np.std(train)\n",
    "    return [mu,sig]\n",
    "def parameterised_function(parameters):\n",
    "    mu = parameters[0]\n",
    "    sig = parameters[1]\n",
    "    return lambda x: np.exp(-0.5*(x - mu)**2/(sig**2))\n",
    "train_data = np.array([[2.0, 4.0, 0.0], [1.0, 5.0, 0.0], [4.0, 2.0, 1.0], [6.0, 0.0, 1.0]])\n",
    "g = naiveBayes(classes, learner, parameterised_function, train_data)\n",
    "test_data = np.array([[2.0, 5.0], [3.0,3.0]])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "for class_value in classes:\n",
    "    print(g[class_value](test_data)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdd405",
   "metadata": {},
   "source": [
    "## Question 1.1\n",
    "### What does the naive Bayes classifier actually return?\n",
    "It returns a dictionary g where g is a pair for each class and function\n",
    "### What do the functions defined inside the main function do?\n",
    "There are two functions defined in the main function:\n",
    "1. `create_g(class_value)`\n",
    "This function returns a specific g function for one class (class_value), where g is later used to calculate the test value for that class.\n",
    "2. `g(test_data)`\n",
    "This is where the test data is tested by calculating the likelihood of the new point for each feature using the naive bayes formula.\n",
    "\n",
    "### What is the role of the inputs to the naiveBayes function, in particular the learner and the parameterized function? \n",
    "- `learner` returns the values [mu, sig] for each class and feature.\n",
    "- `parametetized_function` converts the values mu, sig into a Gaussian function.\n",
    "\n",
    "## Question 1.2\n",
    "## Where does the independence assumption made by the Naive Bayes approach come into the calculation?\n",
    "First, when training features, each feature is learned independently, without regard to other features.\n",
    "Second, during prediction, where the point likelihood is the product of all feature likelihoods (independent).\n",
    "\n",
    "## Question 1.3\n",
    "## What class of functions does the parameterized function in the example represent?\n",
    "It represents a Gaussian function.\n",
    "\n",
    "## Question 1.4\n",
    "See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0238260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species\n",
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: count, dtype: int64\n",
      "Predictions: [2, 1, 1, 2, 2, 2, 1, 1, 0, 2, 0, 0, 2, 2, 0, 2, 1, 0, 0, 0, 1, 0, 1, 2, 2, 1, 1, 1, 1, 0, 2, 2, 1, 0, 2, 0, 0, 0, 0, 2, 1, 0, 1, 2, 1]\n",
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load iris.csv\n",
    "iris = pd.read_csv('iris.csv')\n",
    "\n",
    "# Encode species to integers\n",
    "iris['Species'] = iris['Species'].astype('category').cat.codes\n",
    "print(iris['Species'].value_counts())\n",
    "\n",
    "# Split features and labels\n",
    "# only use features 1-4 (omit numbering and species)\n",
    "X = iris.iloc[:, 1:5].values  # Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n",
    "# Use species as y \n",
    "y = iris['Species'].values\n",
    "\n",
    "# Split into train and test set (randomly)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Prepare train_data in the required format (features + label as last column)\n",
    "# hstack used to stack arrays in sequence horizontally (column wise)\n",
    "# y_train.reshape(-1, 1) is used to change the shape of y_train to be a column\n",
    "train_data = np.hstack([X_train, y_train.reshape(-1, 1)])\n",
    "test_data = X_test\n",
    "\n",
    "# Define classes\n",
    "classes = [0, 1, 2]\n",
    "\n",
    "# Train naive Bayes\n",
    "g = naiveBayes(classes, learner, parameterised_function, train_data)\n",
    "\n",
    "# Predict for test set\n",
    "preds = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    # For each test instance, we will compute the likelihood for each class\n",
    "    # likelihood is calculated by calling the model for each class and taking the likelihood value\n",
    "    # we will use test_data[i:i+1] as input\n",
    "    # g[class_value](test_data[i:i+1]) will return a 2D array\n",
    "    # likelihoods will contain the likelihood values for each class\n",
    "    # we will use np.argmax to automatically get the class with the highest likelihood\n",
    "    # np.argmax will return the index of the maximum value\n",
    "    # we will use that index to get the corresponding class\n",
    "    # classes[np.argmax(likelihoods)] will give the class with the highest likelihood\n",
    "    # we will append that class to preds\n",
    "    likelihoods = [g[class_value](test_data[i:i+1])[0, 0] for class_value in classes]\n",
    "    preds.append(classes[np.argmax(likelihoods)])\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = np.mean(preds == y_test)\n",
    "print(f'Predictions: {preds}')\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff6734",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## Question 2.1\n",
    "See code below. it shows that almost similar accuracy, but slightly better than using Naive Bayes. and even better when using sklearn. \n",
    "## Question 2.2.1 and 2.2.2\n",
    "I cannot produce the modified code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67b727b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tree: tree loss =  0.06666666666666667\n",
      "Basic tree: accuracy =  0.9333333333333333\n",
      "DecisionTreeRegressor: tree loss =  0.022222222222222223\n",
      "DecisionTreeRegressor: accuracy = 0.9666666666666667\n",
      "[[15  0  0]\n",
      " [ 0 14  1]\n",
      " [ 0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" BasicTree.py \"\"\"\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def makedata():\n",
    "#   n_points = 500 # points\n",
    " \n",
    "#   X, y =  make_friedman1(n_samples=n_points, n_features=5, \n",
    "#                          noise=1.0, random_state=100)\n",
    "         \n",
    "#   return train_test_split(X, y, test_size=0.5, random_state=3)\n",
    " \n",
    "def main(X_train, X_test, y_train, y_test):\n",
    "  # X_train, X_test, y_train, y_test = makedata()    \n",
    "  maxdepth = 10 # maximum tree depth             \n",
    "  # Create tree root at depth 0                       \n",
    "  treeRoot = TNode(0, X_train,y_train) \n",
    "       \n",
    "  # Build the regression tree with maximal depth equal to max_depth\n",
    "  Construct_Subtree(treeRoot, maxdepth) \n",
    "    \n",
    "  # Predict\n",
    "  y_hat = np.zeros(len(X_test))\n",
    "  for i in range(len(X_test)):\n",
    "     y_hat[i] = Predict(X_test[i],treeRoot)          \n",
    "    \n",
    "  MSE = np.mean(np.power(y_hat - y_test,2))    \n",
    "  print(\"Basic tree: tree loss = \",  MSE)\n",
    "  print(\"Basic tree: accuracy = \",  np.mean(y_hat == y_test))\n",
    "\n",
    "# tree node\n",
    "class TNode:\n",
    "   def __init__(self, depth, X, y): \n",
    "      self.depth = depth\n",
    "      self.X = X   # matrix of explanatory variables\n",
    "      self.y = y   # vector of response variables\n",
    "      # initialize optimal split parameters\n",
    "      self.j = None\n",
    "      self.xi = None\n",
    "      # initialize children to be None      \n",
    "      self.left = None\n",
    "      self.right = None\n",
    "      # initialize the regional predictor\n",
    "      self.g = None\n",
    "      \n",
    "   def CalculateLoss(self):\n",
    "       if(len(self.y)==0):\n",
    "           return 0\n",
    "       \n",
    "       return np.sum(np.power(self.y- self.y.mean(),2))\n",
    "                    \n",
    "  \n",
    "def Construct_Subtree(node, max_depth):  \n",
    "    if(node.depth == max_depth or len(node.y) == 1):\n",
    "        node.g  = node.y.mean()\n",
    "    else:\n",
    "        j, xi = CalculateOptimalSplit(node)               \n",
    "        node.j = j\n",
    "        node.xi = xi\n",
    "        Xt, yt, Xf, yf = DataSplit(node.X, node.y, j, xi)\n",
    "              \n",
    "        if(len(yt)>0):\n",
    "            node.left = TNode(node.depth+1,Xt,yt)\n",
    "            Construct_Subtree(node.left, max_depth)\n",
    "        \n",
    "        if(len(yf)>0):        \n",
    "            node.right = TNode(node.depth+1, Xf,yf)\n",
    "            Construct_Subtree(node.right, max_depth)      \n",
    "     \n",
    "    return node\n",
    "\n",
    "# split the data-set\n",
    "def DataSplit(X,y,j,xi):\n",
    "    ids = X[:,j]<=xi      \n",
    "    Xt  = X[ids == True,:]\n",
    "    Xf  = X[ids == False,:]\n",
    "    yt  = y[ids == True]\n",
    "    yf  = y[ids == False]\n",
    "    return Xt, yt, Xf, yf             \n",
    "\n",
    "def CalculateOptimalSplit(node):\n",
    "    X = node.X\n",
    "    y = node.y\n",
    "    best_var = 0\n",
    "    best_xi = X[0,best_var]          \n",
    "    best_split_val = node.CalculateLoss()\n",
    "    \n",
    "    m, n  = X.shape\n",
    "    \n",
    "    for j in range(0,n):\n",
    "        for i in range(0,m):\n",
    "            xi = X[i,j]\n",
    "            Xt, yt, Xf, yf = DataSplit(X,y,j,xi)\n",
    "            tmpt = TNode(0, Xt, yt) \n",
    "            tmpf = TNode(0, Xf, yf) \n",
    "            loss_t = tmpt.CalculateLoss()\n",
    "            loss_f = tmpf.CalculateLoss()    \n",
    "            curr_val =  loss_t + loss_f\n",
    "            if (curr_val < best_split_val):\n",
    "                best_split_val = curr_val\n",
    "                best_var = j\n",
    "                best_xi = xi\n",
    "    return best_var,  best_xi\n",
    "\n",
    "\n",
    "def Predict(X,node):\n",
    "    if(node.right == None and node.left != None):\n",
    "        return Predict(X,node.left)\n",
    "    \n",
    "    if(node.right != None and node.left == None):\n",
    "        return Predict(X,node.right)\n",
    "    \n",
    "    if(node.right == None and node.left == None):\n",
    "        return node.g\n",
    "    else:\n",
    "        if(X[node.j] <= node.xi):\n",
    "            return Predict(X,node.left)\n",
    "        else:\n",
    "            return Predict(X,node.right)\n",
    "    \n",
    "main(X_train, X_test, y_train, y_test)  # run the main program\n",
    "\n",
    "# compare with sklearn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# X_train, X_test, y_train, y_test = makedata()    \n",
    "regTree = DecisionTreeRegressor(max_depth = 10, random_state=0)\n",
    "regTree.fit(X_train,y_train)\n",
    "y_hat = regTree.predict(X_test)\n",
    "MSE2 = np.mean(np.power(y_hat - y_test,2))    \n",
    "print(\"DecisionTreeRegressor: tree loss = \",  MSE2)\n",
    "# accuracy\n",
    "accuracy = regTree.score(X_test, y_test)\n",
    "print(\"DecisionTreeRegressor: accuracy =\", accuracy)\n",
    "\n",
    "# Show the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f3f6e",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "## Question 3.1\n",
    "Obviously, apart from datasplit from iris dataset, each of classifiers need different hyperparameter that we can tweak and based on the theories we learned in the lecture. Those are:\n",
    "1. In KNN:\n",
    "    - `n_neighbors`: this parameter sets on how many neighbours will be use to compare (vote) on which class the new point will be. too small number of neightbours will make the decision boundary too noisy (rough and hard to interprete), but too many will make the boundary over smoother (default is 5).\n",
    "    - `p`: controls the distance metric, in theory we use Euclidian distance which also the default for sklearn.\n",
    "2. Logistic Regression:\n",
    "    - `penalty`: this determines which type of regulation we are going to apply (prevent overfitting by pinalizing model coefficient). Default is `l2`, and have other options such as `l1` and `elasticnet` (combination).\n",
    "3. SVM:\n",
    "    - `kernel`: this parameter implements the kernel trick, for example in theory we learn about linear, poly, or gaussian (rbf). the default is gaussian.\n",
    "\n",
    "## Question 3.2\n",
    "See implementation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b001fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Accuracy Comparison ---\n",
      "K-Nearest Neighbors: 0.9778 (97.78%)\n",
      "Logistic Regression: 0.9333 (93.33%)\n",
      "Support Vector Machine: 0.9556 (95.56%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = pd.read_csv('iris.csv')\n",
    "iris['Species'] = iris['Species'].astype('category').cat.codes\n",
    "X = iris.iloc[:, 1:5].values\n",
    "y = iris['Species'].values\n",
    "# Use the same data split as your previous examples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 2. Initialize the classifiers with reasonable parameters\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=200)\n",
    "svm = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "\n",
    "classifiers = {\n",
    "    \"K-Nearest Neighbors\": knn,\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Support Vector Machine\": svm\n",
    "}\n",
    "\n",
    "print(\"\\n--- Accuracy Comparison ---\")\n",
    "\n",
    "# 3. Fit models and calculate accuracy\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name}: {accuracy:.4f} ({accuracy:.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
